# perturbation-internalization
Internalize beneficial perturbations to image classifier models in an effort to increase feature interpretability.

Just as we see in [Szegedy et al.](https://arxiv.org/abs/1906.00945) with models becoming increasingly performant and robust when trained on adversarially perturbed data, we can similarly attempt to internalize the benefits of intentionally *helpful* perturbations like those described in [Exploring Visual Prompts for Adapting Large-Scale Models](https://arxiv.org/abs/2203.17274) by Bahng et al. Further information can be found in the attached `site` folder.

My modifications of the original visual prompts paper (used in this work) can be found [here](https://github.com/Diatom33/visual_prompting).
